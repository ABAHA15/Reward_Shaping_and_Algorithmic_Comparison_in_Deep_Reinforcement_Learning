Reward Shaping and Algorithmic Comparison in Deep Reinforcement Learning


ğŸ“Œ Project Overview:

This repository contains the complete implementation and experimental study for the project â€œReward Shaping and Algorithmic Comparison in Deep Reinforcement Learningâ€, conducted as part of an academic course on Reinforcement Learning.

The project is divided into two major parts:

Part 1: Comparative analysis of multiple Deep Reinforcement Learning (DRL) algorithms on a standard benchmark environment (CartPole-v1), with a focused study on the impact of reward shaping.

Part 2: Design and evaluation of custom Gym-compatible environments with graphical user interfaces (GUI), highlighting agent behavior in deterministic and dynamic settings.

The project emphasizes understanding, interpretation, and critical analysis, rather than blind application of libraries.


ğŸ¯ Objectives:

Compare value-based, policy-gradient, and actorâ€“critic algorithms under identical conditions.
Analyze how reward shaping affects learning speed, stability, and convergence.
Design custom non-trivial environments that require genuine decision-making.
Demonstrate learned policies through visual rollouts and GUI-based demos.
Analyze both successes and failures of Deep RL algorithms.


ğŸ§  Algorithms Implemented:

Part 1 â€“ Benchmark Environment (CartPole-v1):

DQN
Double DQN
REINFORCE (from scratch, PyTorch)
Advantage Actorâ€“Critic (A2C)
Proximal Policy Optimization (PPO)
Each algorithm is trained using:
Baseline rewards
Shaped rewards
Learning curves are generated and analyzed for all configurations.


Part 2 â€“ Custom Environments:

GridWorld (5Ã—5) with fixed obstacles
DQN with sparse rewards
DQN with shaped rewards
Custom GUI visualization
Pacman-style dynamic environment
Moving adversary (ghost)
Stochastic dynamics
PPO agent with adaptive behavior
Custom GUI visualization



ğŸ—ï¸ Repository Structure:
.
â”œâ”€â”€ cartpole/
â”‚   â”œâ”€â”€ train_dqn_baseline.py
â”‚   â”œâ”€â”€ train_dqn_shaped.py
â”‚   â”œâ”€â”€ train_double_dqn_baseline.py
â”‚   â”œâ”€â”€ train_double_dqn_shaped.py
â”‚   â”œâ”€â”€ train_reinforce_baseline.py
â”‚   â”œâ”€â”€ train_reinforce_shaped.py
â”‚   â”œâ”€â”€ train_a2c.py
â”‚   â”œâ”€â”€ train_ppo.py
â”‚   â””â”€â”€ plots/
â”‚       â”œâ”€â”€ baseline_reward_curve_*.png
â”‚       â””â”€â”€ shaped_reward_curve_*.png
â”‚
â”œâ”€â”€ gridworld/
â”‚   â”œâ”€â”€ gridworld_env.py
â”‚   â”œâ”€â”€ train_dqn_sparse.py
â”‚   â”œâ”€â”€ train_dqn_shaped.py
â”‚   â”œâ”€â”€ gridworld_gui.py
â”‚   â””â”€â”€ plots/
â”‚       â”œâ”€â”€ gridworld_sparse_reward_dqn.png
â”‚       â””â”€â”€ gridworld_shaped_reward_dqn.png
â”‚
â”œâ”€â”€ pacman_env/
â”‚   â”œâ”€â”€ pacman_env.py
â”‚   â”œâ”€â”€ train_ppo_pacman.py
â”‚   â”œâ”€â”€ pacman_gui.py
â”‚   â””â”€â”€ plots/
â”‚       â””â”€â”€ reward_curve_ppo_pacman.png
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ dqn_cartpole.zip
â”‚   â”œâ”€â”€ double_dqn_cartpole.zip
â”‚   â”œâ”€â”€ ppo_cartpole.zip
â”‚   â””â”€â”€ ppo_pacman.zip
â”‚
â”œâ”€â”€ report/
â”‚   â”œâ”€â”€ Project_Report.pdf
â”‚   â””â”€â”€ latex_source/
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt


âš™ï¸ Setup Instructions:


1ï¸âƒ£ Create Virtual Environment

python -m venv rl_env
rl_env\Scripts\activate

2ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

Key Libraries Used:

Python 3.9
Gymnasium
Stable-Baselines3
PyTorch
NumPy
Matplotlib
Pygame (for GUI)

â–¶ï¸ Running the Experiments:

CartPole (Example)
python train_double_dqn_baseline.py
python train_double_dqn_shaped.py

GridWorld GUI
python gridworld_gui.py

Pacman-Style Environment GUI
python pacman_gui.py


Pre-trained models are provided to enable real-time demos without retraining.


ğŸ“Š Results and Analysis:

Learning curves compare baseline vs shaped rewards.
Sample efficiency, stability, and convergence are analyzed.
Failures (instability, suboptimal convergence, conservative policies) are explicitly discussed.
GUI rollouts provide qualitative insight into learned behavior.
For detailed analysis, refer to the Project Report (PDF) in the report/ directory.


ğŸ¥ Demos:

The project includes:

Saved trained models for live demos.
GUI-based visualizations for custom environments.
Video recordings of trained agents (for presentation use).


ğŸ“Œ Key Takeaways:

Reward shaping improves early learning but may introduce bias.
PPO provides the most stable performance across environments.
No single RL algorithm is universally optimal.
Custom environment design is as important as algorithm choice.


ğŸš€ Future Extensions:

Continuous control (SAC, TD3)
Partial observability and memory-based agents
Multi-agent extensions
Automated reward design
Robustness and generalization studies


ğŸ“œ License:

This project is intended for academic and educational use.

